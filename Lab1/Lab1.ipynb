{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206a4e4a",
   "metadata": {},
   "source": [
    "# Hand Gesture Classification Using Deep Learning\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "In ML1, I worked on a multi-class hand gesture classification problem using classical machine learning models such as SVM and Random Forest.\n",
    "\n",
    "Each sample consists of 21 hand landmarks extracted from MediaPipe.  \n",
    "Each landmark has (x, y, z) coordinates, so the total number of input features is:\n",
    "\n",
    "21 × 3 = 63 features.\n",
    "\n",
    "The goal is to predict the gesture label (e.g., fist, call, dislike, etc.).\n",
    "\n",
    "This is a supervised multi-class classification problem where:\n",
    "\n",
    "X ∈ R^63  \n",
    "y ∈ {0, 1, ..., K-1}\n",
    "\n",
    "In this notebook, I reimplement the same problem using a Deep Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27996fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29d1f9c",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. **Label Encoding** — gesture names are encoded into integer class labels.\n",
    "\n",
    "2. **Translation Normalization** — X and Y coordinates are shifted so that\n",
    "   landmark 0 (wrist) becomes the origin. This makes the representation\n",
    "   position-invariant.\n",
    "\n",
    "3. **Scale Normalization** — X and Y coordinates are divided by the mean\n",
    "   distance from the wrist to the 4 fingertips (index, middle, ring, pinky).\n",
    "   This makes the representation scale-invariant regardless of hand size or\n",
    "   distance from the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97bf0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"/Users/ahmedtarek/Developer/Python/DL/hand_landmarks_data.csv\")\n",
    "\n",
    "X = df.drop('label', axis=1).copy()\n",
    "y = df['label']\n",
    "\n",
    "x_cols = list(range(0, X.shape[1], 3))\n",
    "y_cols = [c + 1 for c in x_cols]\n",
    "z_cols = [c + 2 for c in x_cols]\n",
    "\n",
    "# 1️⃣ Translation normalization (X and Y only)\n",
    "X.iloc[:, x_cols] = X.iloc[:, x_cols].sub(X.iloc[:, 0], axis=0)\n",
    "X.iloc[:, y_cols] = X.iloc[:, y_cols].sub(X.iloc[:, 1], axis=0)\n",
    "\n",
    "# 2️⃣ Scale normalization using 4 fingertips (no thumb)\n",
    "fingertip_indices = [8, 12, 16, 20]\n",
    "\n",
    "fingertip_distances = []\n",
    "for tip in fingertip_indices:\n",
    "    col_x = tip * 3\n",
    "    col_y = tip * 3 + 1\n",
    "    dist = np.sqrt(X.iloc[:, col_x]**2 + X.iloc[:, col_y]**2)\n",
    "    fingertip_distances.append(dist)\n",
    "\n",
    "div = np.mean(fingertip_distances, axis=0)\n",
    "div = np.where(div < 1e-6, 1e-6, div)\n",
    "\n",
    "X.iloc[:, x_cols] = X.iloc[:, x_cols].div(div, axis=0)\n",
    "X.iloc[:, y_cols] = X.iloc[:, y_cols].div(div, axis=0)\n",
    "\n",
    "X_scaled = X.values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adecc141",
   "metadata": {},
   "source": [
    "## Train / Validation / Test Split\n",
    "\n",
    "The dataset is split into:\n",
    "\n",
    "- Training set → used to update model parameters.\n",
    "- Validation set → used to monitor generalization and apply early stopping.\n",
    "- Test set → used only once at the end for final evaluation.\n",
    "\n",
    "This separation ensures that the test set remains completely unseen during training.  \n",
    "It allows us to measure the true generalization performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8062ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_val = torch.LongTensor(y_val)\n",
    "\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cadf215",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "I implement a fully connected neural network (Multi-Layer Perceptron).\n",
    "\n",
    "Structure:\n",
    "- Input layer: 63 neurons (one per feature)\n",
    "- Hidden Layer 1: 128 neurons + ReLU\n",
    "- Hidden Layer 2: 64 neurons + ReLU\n",
    "- Output layer: K neurons (number of gesture classes)\n",
    "\n",
    "Why ReLU?\n",
    "\n",
    "ReLU(x) = max(0, x)\n",
    "\n",
    "ReLU helps reduce the vanishing gradient problem and allows deeper models to train more efficiently compared to sigmoid or tanh.\n",
    "\n",
    "The model learns nonlinear transformations of the input features, which allows it to capture complex decision boundaries compared to classical linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6e1ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(GestureNN, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = GestureNN(input_size=63, num_classes=len(le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8ec77",
   "metadata": {},
   "source": [
    "## Loss Function and Optimization\n",
    "\n",
    "For multi-class classification, I use CrossEntropyLoss.\n",
    "\n",
    "Cross-entropy measures the difference between predicted class probabilities and the true class label.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "L = - Σ y_i log(ŷ_i)\n",
    "\n",
    "This encourages the model to assign high probability to the correct class.\n",
    "\n",
    "For optimization, I use Adam.\n",
    "\n",
    "Adam combines:\n",
    "- Momentum (to accelerate convergence)\n",
    "- Adaptive learning rates (to handle different parameter scales)\n",
    "\n",
    "This makes training more stable and faster compared to standard gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7c89444",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986c43f0",
   "metadata": {},
   "source": [
    "## Training Process with Early Stopping\n",
    "\n",
    "During training, each batch of data goes through the following steps:\n",
    "\n",
    "1. **Forward pass** – compute predictions.  \n",
    "2. **Compute loss** – measure the difference between predictions and true labels.  \n",
    "3. **Backward pass** – compute gradients using backpropagation.  \n",
    "4. **Update weights** – adjust model parameters using the Adam optimizer according to the gradient descent rule:\n",
    "\n",
    "\\[\n",
    "\\theta = \\theta - \\alpha \\nabla L(\\theta)\n",
    "\\]\n",
    "\n",
    "Where:  \n",
    "- **θ** represents the model parameters.  \n",
    "- **α** is the learning rate.  \n",
    "- **∇L(θ)** is the gradient of the loss with respect to the parameters.  \n",
    "\n",
    "This process is repeated for multiple epochs until the model converges.\n",
    "\n",
    "### Early Stopping (Regularization)\n",
    "\n",
    "To prevent overfitting, we implement **early stopping** based on validation loss:\n",
    "\n",
    "- Training loss usually decreases continuously.  \n",
    "- Validation loss decreases initially but may start increasing once overfitting begins.  \n",
    "\n",
    "We monitor the validation loss after each epoch. If it does not improve for a fixed number of epochs (called **patience**), training stops, and the best model (with the lowest validation loss) is restored.  \n",
    "\n",
    "By selecting the parameters that minimize validation loss rather than just training loss, early stopping acts as an **implicit regularization method**, helping the model generalize better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ef658189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] Train Loss: 2.8537 Val Loss: 2.7916 Val Acc: 0.1797\n",
      "Epoch [2/300] Train Loss: 2.6994 Val Loss: 2.5534 Val Acc: 0.2262\n",
      "Epoch [3/300] Train Loss: 2.4532 Val Loss: 2.3019 Val Acc: 0.3243\n",
      "Epoch [4/300] Train Loss: 2.2390 Val Loss: 2.0708 Val Acc: 0.3794\n",
      "Epoch [5/300] Train Loss: 2.0330 Val Loss: 1.8456 Val Acc: 0.4363\n",
      "Epoch [6/300] Train Loss: 1.8500 Val Loss: 1.6598 Val Acc: 0.4531\n",
      "Epoch [7/300] Train Loss: 1.7064 Val Loss: 1.5115 Val Acc: 0.5510\n",
      "Epoch [8/300] Train Loss: 1.5824 Val Loss: 1.3906 Val Acc: 0.5669\n",
      "Epoch [9/300] Train Loss: 1.4817 Val Loss: 1.2924 Val Acc: 0.6398\n",
      "Epoch [10/300] Train Loss: 1.3995 Val Loss: 1.2148 Val Acc: 0.6432\n",
      "Epoch [11/300] Train Loss: 1.3323 Val Loss: 1.1509 Val Acc: 0.6788\n",
      "Epoch [12/300] Train Loss: 1.2724 Val Loss: 1.0938 Val Acc: 0.7099\n",
      "Epoch [13/300] Train Loss: 1.2194 Val Loss: 1.0410 Val Acc: 0.7318\n",
      "Epoch [14/300] Train Loss: 1.1736 Val Loss: 0.9944 Val Acc: 0.7341\n",
      "Epoch [15/300] Train Loss: 1.1209 Val Loss: 0.9468 Val Acc: 0.7632\n",
      "Epoch [16/300] Train Loss: 1.0810 Val Loss: 0.9017 Val Acc: 0.7827\n",
      "Epoch [17/300] Train Loss: 1.0411 Val Loss: 0.8587 Val Acc: 0.7899\n",
      "Epoch [18/300] Train Loss: 1.0013 Val Loss: 0.8154 Val Acc: 0.8073\n",
      "Epoch [19/300] Train Loss: 0.9583 Val Loss: 0.7747 Val Acc: 0.8117\n",
      "Epoch [20/300] Train Loss: 0.9123 Val Loss: 0.7348 Val Acc: 0.8271\n",
      "Epoch [21/300] Train Loss: 0.8821 Val Loss: 0.6967 Val Acc: 0.8325\n",
      "Epoch [22/300] Train Loss: 0.8451 Val Loss: 0.6605 Val Acc: 0.8489\n",
      "Epoch [23/300] Train Loss: 0.8067 Val Loss: 0.6286 Val Acc: 0.8522\n",
      "Epoch [24/300] Train Loss: 0.7787 Val Loss: 0.5975 Val Acc: 0.8543\n",
      "Epoch [25/300] Train Loss: 0.7498 Val Loss: 0.5703 Val Acc: 0.8689\n",
      "Epoch [26/300] Train Loss: 0.7226 Val Loss: 0.5450 Val Acc: 0.8678\n",
      "Epoch [27/300] Train Loss: 0.6982 Val Loss: 0.5211 Val Acc: 0.8707\n",
      "Epoch [28/300] Train Loss: 0.6705 Val Loss: 0.5011 Val Acc: 0.8722\n",
      "Epoch [29/300] Train Loss: 0.6471 Val Loss: 0.4801 Val Acc: 0.8780\n",
      "Epoch [30/300] Train Loss: 0.6325 Val Loss: 0.4626 Val Acc: 0.8873\n",
      "Epoch [31/300] Train Loss: 0.6079 Val Loss: 0.4467 Val Acc: 0.8878\n",
      "Epoch [32/300] Train Loss: 0.5937 Val Loss: 0.4298 Val Acc: 0.8951\n",
      "Epoch [33/300] Train Loss: 0.5711 Val Loss: 0.4147 Val Acc: 0.8938\n",
      "Epoch [34/300] Train Loss: 0.5628 Val Loss: 0.4034 Val Acc: 0.8946\n",
      "Epoch [35/300] Train Loss: 0.5423 Val Loss: 0.3903 Val Acc: 0.8930\n",
      "Epoch [36/300] Train Loss: 0.5314 Val Loss: 0.3781 Val Acc: 0.8951\n",
      "Epoch [37/300] Train Loss: 0.5172 Val Loss: 0.3657 Val Acc: 0.9000\n",
      "Epoch [38/300] Train Loss: 0.5045 Val Loss: 0.3558 Val Acc: 0.9005\n",
      "Epoch [39/300] Train Loss: 0.4940 Val Loss: 0.3457 Val Acc: 0.9037\n",
      "Epoch [40/300] Train Loss: 0.4851 Val Loss: 0.3381 Val Acc: 0.9021\n",
      "Epoch [41/300] Train Loss: 0.4714 Val Loss: 0.3287 Val Acc: 0.9135\n",
      "Epoch [42/300] Train Loss: 0.4556 Val Loss: 0.3196 Val Acc: 0.9081\n",
      "Epoch [43/300] Train Loss: 0.4453 Val Loss: 0.3109 Val Acc: 0.9127\n",
      "Epoch [44/300] Train Loss: 0.4345 Val Loss: 0.3033 Val Acc: 0.9120\n",
      "Epoch [45/300] Train Loss: 0.4300 Val Loss: 0.2964 Val Acc: 0.9185\n",
      "Epoch [46/300] Train Loss: 0.4256 Val Loss: 0.2909 Val Acc: 0.9185\n",
      "Epoch [47/300] Train Loss: 0.4151 Val Loss: 0.2845 Val Acc: 0.9198\n",
      "Epoch [48/300] Train Loss: 0.4074 Val Loss: 0.2779 Val Acc: 0.9221\n",
      "Epoch [49/300] Train Loss: 0.3965 Val Loss: 0.2716 Val Acc: 0.9242\n",
      "Epoch [50/300] Train Loss: 0.3963 Val Loss: 0.2669 Val Acc: 0.9239\n",
      "Epoch [51/300] Train Loss: 0.3849 Val Loss: 0.2613 Val Acc: 0.9247\n",
      "Epoch [52/300] Train Loss: 0.3756 Val Loss: 0.2561 Val Acc: 0.9278\n",
      "Epoch [53/300] Train Loss: 0.3708 Val Loss: 0.2500 Val Acc: 0.9281\n",
      "Epoch [54/300] Train Loss: 0.3672 Val Loss: 0.2447 Val Acc: 0.9317\n",
      "Epoch [55/300] Train Loss: 0.3646 Val Loss: 0.2407 Val Acc: 0.9330\n",
      "Epoch [56/300] Train Loss: 0.3581 Val Loss: 0.2375 Val Acc: 0.9320\n",
      "Epoch [57/300] Train Loss: 0.3484 Val Loss: 0.2323 Val Acc: 0.9372\n",
      "Epoch [58/300] Train Loss: 0.3493 Val Loss: 0.2282 Val Acc: 0.9382\n",
      "Epoch [59/300] Train Loss: 0.3374 Val Loss: 0.2241 Val Acc: 0.9400\n",
      "Epoch [60/300] Train Loss: 0.3293 Val Loss: 0.2191 Val Acc: 0.9424\n",
      "Epoch [61/300] Train Loss: 0.3345 Val Loss: 0.2152 Val Acc: 0.9426\n",
      "Epoch [62/300] Train Loss: 0.3214 Val Loss: 0.2100 Val Acc: 0.9457\n",
      "Epoch [63/300] Train Loss: 0.3169 Val Loss: 0.2061 Val Acc: 0.9449\n",
      "Epoch [64/300] Train Loss: 0.3169 Val Loss: 0.2032 Val Acc: 0.9470\n",
      "Epoch [65/300] Train Loss: 0.3072 Val Loss: 0.1992 Val Acc: 0.9481\n",
      "Epoch [66/300] Train Loss: 0.3004 Val Loss: 0.1952 Val Acc: 0.9520\n",
      "Epoch [67/300] Train Loss: 0.2968 Val Loss: 0.1912 Val Acc: 0.9520\n",
      "Epoch [68/300] Train Loss: 0.3001 Val Loss: 0.1879 Val Acc: 0.9530\n",
      "Epoch [69/300] Train Loss: 0.2871 Val Loss: 0.1833 Val Acc: 0.9543\n",
      "Epoch [70/300] Train Loss: 0.2866 Val Loss: 0.1797 Val Acc: 0.9551\n",
      "Epoch [71/300] Train Loss: 0.2796 Val Loss: 0.1770 Val Acc: 0.9538\n",
      "Epoch [72/300] Train Loss: 0.2769 Val Loss: 0.1733 Val Acc: 0.9577\n",
      "Epoch [73/300] Train Loss: 0.2761 Val Loss: 0.1701 Val Acc: 0.9561\n",
      "Epoch [74/300] Train Loss: 0.2649 Val Loss: 0.1658 Val Acc: 0.9579\n",
      "Epoch [75/300] Train Loss: 0.2628 Val Loss: 0.1619 Val Acc: 0.9587\n",
      "Epoch [76/300] Train Loss: 0.2651 Val Loss: 0.1593 Val Acc: 0.9595\n",
      "Epoch [77/300] Train Loss: 0.2525 Val Loss: 0.1549 Val Acc: 0.9634\n",
      "Epoch [78/300] Train Loss: 0.2559 Val Loss: 0.1537 Val Acc: 0.9610\n",
      "Epoch [79/300] Train Loss: 0.2505 Val Loss: 0.1509 Val Acc: 0.9655\n",
      "Epoch [80/300] Train Loss: 0.2461 Val Loss: 0.1478 Val Acc: 0.9647\n",
      "Epoch [81/300] Train Loss: 0.2422 Val Loss: 0.1442 Val Acc: 0.9657\n",
      "Epoch [82/300] Train Loss: 0.2440 Val Loss: 0.1422 Val Acc: 0.9670\n",
      "Epoch [83/300] Train Loss: 0.2345 Val Loss: 0.1394 Val Acc: 0.9652\n",
      "Epoch [84/300] Train Loss: 0.2340 Val Loss: 0.1367 Val Acc: 0.9665\n",
      "Epoch [85/300] Train Loss: 0.2281 Val Loss: 0.1335 Val Acc: 0.9694\n",
      "Epoch [86/300] Train Loss: 0.2289 Val Loss: 0.1316 Val Acc: 0.9701\n",
      "Epoch [87/300] Train Loss: 0.2266 Val Loss: 0.1304 Val Acc: 0.9694\n",
      "Epoch [88/300] Train Loss: 0.2231 Val Loss: 0.1273 Val Acc: 0.9701\n",
      "Epoch [89/300] Train Loss: 0.2175 Val Loss: 0.1258 Val Acc: 0.9712\n",
      "Epoch [90/300] Train Loss: 0.2171 Val Loss: 0.1247 Val Acc: 0.9714\n",
      "Epoch [91/300] Train Loss: 0.2148 Val Loss: 0.1211 Val Acc: 0.9725\n",
      "Epoch [92/300] Train Loss: 0.2091 Val Loss: 0.1180 Val Acc: 0.9727\n",
      "Epoch [93/300] Train Loss: 0.2060 Val Loss: 0.1170 Val Acc: 0.9727\n",
      "Epoch [94/300] Train Loss: 0.2088 Val Loss: 0.1158 Val Acc: 0.9730\n",
      "Epoch [95/300] Train Loss: 0.2009 Val Loss: 0.1138 Val Acc: 0.9730\n",
      "Epoch [96/300] Train Loss: 0.1983 Val Loss: 0.1116 Val Acc: 0.9735\n",
      "Epoch [97/300] Train Loss: 0.1990 Val Loss: 0.1099 Val Acc: 0.9738\n",
      "Epoch [98/300] Train Loss: 0.1950 Val Loss: 0.1083 Val Acc: 0.9748\n",
      "Epoch [99/300] Train Loss: 0.1930 Val Loss: 0.1071 Val Acc: 0.9759\n",
      "Epoch [100/300] Train Loss: 0.1992 Val Loss: 0.1063 Val Acc: 0.9751\n",
      "Epoch [101/300] Train Loss: 0.1898 Val Loss: 0.1045 Val Acc: 0.9743\n",
      "Epoch [102/300] Train Loss: 0.1861 Val Loss: 0.1031 Val Acc: 0.9764\n",
      "Epoch [103/300] Train Loss: 0.1876 Val Loss: 0.1023 Val Acc: 0.9759\n",
      "Epoch [104/300] Train Loss: 0.1862 Val Loss: 0.1001 Val Acc: 0.9769\n",
      "Epoch [105/300] Train Loss: 0.1840 Val Loss: 0.1001 Val Acc: 0.9766\n",
      "Epoch [106/300] Train Loss: 0.1792 Val Loss: 0.0978 Val Acc: 0.9771\n",
      "Epoch [107/300] Train Loss: 0.1807 Val Loss: 0.0969 Val Acc: 0.9779\n",
      "Epoch [108/300] Train Loss: 0.1770 Val Loss: 0.0957 Val Acc: 0.9784\n",
      "Epoch [109/300] Train Loss: 0.1723 Val Loss: 0.0947 Val Acc: 0.9782\n",
      "Epoch [110/300] Train Loss: 0.1759 Val Loss: 0.0933 Val Acc: 0.9790\n",
      "Epoch [111/300] Train Loss: 0.1713 Val Loss: 0.0926 Val Acc: 0.9782\n",
      "Epoch [112/300] Train Loss: 0.1694 Val Loss: 0.0914 Val Acc: 0.9782\n",
      "Epoch [113/300] Train Loss: 0.1674 Val Loss: 0.0911 Val Acc: 0.9795\n",
      "Epoch [114/300] Train Loss: 0.1643 Val Loss: 0.0902 Val Acc: 0.9790\n",
      "Epoch [115/300] Train Loss: 0.1634 Val Loss: 0.0887 Val Acc: 0.9797\n",
      "Epoch [116/300] Train Loss: 0.1651 Val Loss: 0.0878 Val Acc: 0.9795\n",
      "Epoch [117/300] Train Loss: 0.1636 Val Loss: 0.0872 Val Acc: 0.9795\n",
      "Epoch [118/300] Train Loss: 0.1609 Val Loss: 0.0861 Val Acc: 0.9792\n",
      "Epoch [119/300] Train Loss: 0.1590 Val Loss: 0.0852 Val Acc: 0.9805\n",
      "Epoch [120/300] Train Loss: 0.1592 Val Loss: 0.0854 Val Acc: 0.9803\n",
      "Early stopping counter: 1/15\n",
      "Epoch [121/300] Train Loss: 0.1557 Val Loss: 0.0834 Val Acc: 0.9797\n",
      "Epoch [122/300] Train Loss: 0.1557 Val Loss: 0.0832 Val Acc: 0.9805\n",
      "Epoch [123/300] Train Loss: 0.1539 Val Loss: 0.0824 Val Acc: 0.9810\n",
      "Epoch [124/300] Train Loss: 0.1493 Val Loss: 0.0821 Val Acc: 0.9797\n",
      "Epoch [125/300] Train Loss: 0.1527 Val Loss: 0.0808 Val Acc: 0.9805\n",
      "Epoch [126/300] Train Loss: 0.1510 Val Loss: 0.0807 Val Acc: 0.9805\n",
      "Epoch [127/300] Train Loss: 0.1473 Val Loss: 0.0795 Val Acc: 0.9803\n",
      "Epoch [128/300] Train Loss: 0.1499 Val Loss: 0.0791 Val Acc: 0.9810\n",
      "Epoch [129/300] Train Loss: 0.1493 Val Loss: 0.0781 Val Acc: 0.9816\n",
      "Epoch [130/300] Train Loss: 0.1506 Val Loss: 0.0783 Val Acc: 0.9821\n",
      "Early stopping counter: 1/15\n",
      "Epoch [131/300] Train Loss: 0.1440 Val Loss: 0.0779 Val Acc: 0.9808\n",
      "Epoch [132/300] Train Loss: 0.1437 Val Loss: 0.0773 Val Acc: 0.9816\n",
      "Epoch [133/300] Train Loss: 0.1424 Val Loss: 0.0768 Val Acc: 0.9816\n",
      "Epoch [134/300] Train Loss: 0.1416 Val Loss: 0.0763 Val Acc: 0.9808\n",
      "Epoch [135/300] Train Loss: 0.1412 Val Loss: 0.0752 Val Acc: 0.9829\n",
      "Epoch [136/300] Train Loss: 0.1418 Val Loss: 0.0756 Val Acc: 0.9810\n",
      "Early stopping counter: 1/15\n",
      "Epoch [137/300] Train Loss: 0.1381 Val Loss: 0.0744 Val Acc: 0.9821\n",
      "Epoch [138/300] Train Loss: 0.1426 Val Loss: 0.0744 Val Acc: 0.9810\n",
      "Early stopping counter: 1/15\n",
      "Epoch [139/300] Train Loss: 0.1376 Val Loss: 0.0748 Val Acc: 0.9813\n",
      "Early stopping counter: 2/15\n",
      "Epoch [140/300] Train Loss: 0.1383 Val Loss: 0.0734 Val Acc: 0.9818\n",
      "Epoch [141/300] Train Loss: 0.1358 Val Loss: 0.0725 Val Acc: 0.9831\n",
      "Epoch [142/300] Train Loss: 0.1370 Val Loss: 0.0738 Val Acc: 0.9816\n",
      "Early stopping counter: 1/15\n",
      "Epoch [143/300] Train Loss: 0.1350 Val Loss: 0.0732 Val Acc: 0.9810\n",
      "Early stopping counter: 2/15\n",
      "Epoch [144/300] Train Loss: 0.1296 Val Loss: 0.0724 Val Acc: 0.9821\n",
      "Epoch [145/300] Train Loss: 0.1328 Val Loss: 0.0717 Val Acc: 0.9831\n",
      "Epoch [146/300] Train Loss: 0.1304 Val Loss: 0.0715 Val Acc: 0.9829\n",
      "Epoch [147/300] Train Loss: 0.1288 Val Loss: 0.0704 Val Acc: 0.9829\n",
      "Epoch [148/300] Train Loss: 0.1294 Val Loss: 0.0701 Val Acc: 0.9831\n",
      "Epoch [149/300] Train Loss: 0.1296 Val Loss: 0.0702 Val Acc: 0.9826\n",
      "Early stopping counter: 1/15\n",
      "Epoch [150/300] Train Loss: 0.1343 Val Loss: 0.0703 Val Acc: 0.9821\n",
      "Early stopping counter: 2/15\n",
      "Epoch [151/300] Train Loss: 0.1262 Val Loss: 0.0692 Val Acc: 0.9842\n",
      "Epoch [152/300] Train Loss: 0.1313 Val Loss: 0.0692 Val Acc: 0.9826\n",
      "Early stopping counter: 1/15\n",
      "Epoch [153/300] Train Loss: 0.1255 Val Loss: 0.0697 Val Acc: 0.9826\n",
      "Early stopping counter: 2/15\n",
      "Epoch [154/300] Train Loss: 0.1251 Val Loss: 0.0681 Val Acc: 0.9831\n",
      "Epoch [155/300] Train Loss: 0.1257 Val Loss: 0.0691 Val Acc: 0.9829\n",
      "Early stopping counter: 1/15\n",
      "Epoch [156/300] Train Loss: 0.1244 Val Loss: 0.0683 Val Acc: 0.9831\n",
      "Early stopping counter: 2/15\n",
      "Epoch [157/300] Train Loss: 0.1194 Val Loss: 0.0678 Val Acc: 0.9842\n",
      "Epoch [158/300] Train Loss: 0.1221 Val Loss: 0.0672 Val Acc: 0.9852\n",
      "Epoch [159/300] Train Loss: 0.1270 Val Loss: 0.0665 Val Acc: 0.9844\n",
      "Epoch [160/300] Train Loss: 0.1230 Val Loss: 0.0664 Val Acc: 0.9844\n",
      "Epoch [161/300] Train Loss: 0.1205 Val Loss: 0.0668 Val Acc: 0.9844\n",
      "Early stopping counter: 1/15\n",
      "Epoch [162/300] Train Loss: 0.1193 Val Loss: 0.0664 Val Acc: 0.9829\n",
      "Epoch [163/300] Train Loss: 0.1190 Val Loss: 0.0655 Val Acc: 0.9839\n",
      "Epoch [164/300] Train Loss: 0.1189 Val Loss: 0.0664 Val Acc: 0.9821\n",
      "Early stopping counter: 1/15\n",
      "Epoch [165/300] Train Loss: 0.1208 Val Loss: 0.0654 Val Acc: 0.9836\n",
      "Epoch [166/300] Train Loss: 0.1195 Val Loss: 0.0656 Val Acc: 0.9844\n",
      "Early stopping counter: 1/15\n",
      "Epoch [167/300] Train Loss: 0.1140 Val Loss: 0.0651 Val Acc: 0.9842\n",
      "Epoch [168/300] Train Loss: 0.1171 Val Loss: 0.0645 Val Acc: 0.9852\n",
      "Epoch [169/300] Train Loss: 0.1162 Val Loss: 0.0651 Val Acc: 0.9829\n",
      "Early stopping counter: 1/15\n",
      "Epoch [170/300] Train Loss: 0.1194 Val Loss: 0.0642 Val Acc: 0.9847\n",
      "Epoch [171/300] Train Loss: 0.1210 Val Loss: 0.0639 Val Acc: 0.9847\n",
      "Epoch [172/300] Train Loss: 0.1142 Val Loss: 0.0639 Val Acc: 0.9847\n",
      "Epoch [173/300] Train Loss: 0.1133 Val Loss: 0.0639 Val Acc: 0.9836\n",
      "Epoch [174/300] Train Loss: 0.1127 Val Loss: 0.0635 Val Acc: 0.9839\n",
      "Epoch [175/300] Train Loss: 0.1111 Val Loss: 0.0629 Val Acc: 0.9844\n",
      "Epoch [176/300] Train Loss: 0.1136 Val Loss: 0.0633 Val Acc: 0.9847\n",
      "Early stopping counter: 1/15\n",
      "Epoch [177/300] Train Loss: 0.1120 Val Loss: 0.0632 Val Acc: 0.9849\n",
      "Early stopping counter: 2/15\n",
      "Epoch [178/300] Train Loss: 0.1085 Val Loss: 0.0631 Val Acc: 0.9852\n",
      "Early stopping counter: 3/15\n",
      "Epoch [179/300] Train Loss: 0.1140 Val Loss: 0.0626 Val Acc: 0.9852\n",
      "Epoch [180/300] Train Loss: 0.1128 Val Loss: 0.0630 Val Acc: 0.9844\n",
      "Early stopping counter: 1/15\n",
      "Epoch [181/300] Train Loss: 0.1076 Val Loss: 0.0623 Val Acc: 0.9849\n",
      "Epoch [182/300] Train Loss: 0.1102 Val Loss: 0.0621 Val Acc: 0.9844\n",
      "Epoch [183/300] Train Loss: 0.1066 Val Loss: 0.0621 Val Acc: 0.9842\n",
      "Epoch [184/300] Train Loss: 0.1119 Val Loss: 0.0620 Val Acc: 0.9844\n",
      "Epoch [185/300] Train Loss: 0.1095 Val Loss: 0.0618 Val Acc: 0.9852\n",
      "Epoch [186/300] Train Loss: 0.1089 Val Loss: 0.0610 Val Acc: 0.9857\n",
      "Epoch [187/300] Train Loss: 0.1056 Val Loss: 0.0613 Val Acc: 0.9849\n",
      "Early stopping counter: 1/15\n",
      "Epoch [188/300] Train Loss: 0.1081 Val Loss: 0.0610 Val Acc: 0.9857\n",
      "Epoch [189/300] Train Loss: 0.1077 Val Loss: 0.0609 Val Acc: 0.9852\n",
      "Epoch [190/300] Train Loss: 0.1091 Val Loss: 0.0613 Val Acc: 0.9855\n",
      "Early stopping counter: 1/15\n",
      "Epoch [191/300] Train Loss: 0.1103 Val Loss: 0.0611 Val Acc: 0.9852\n",
      "Early stopping counter: 2/15\n",
      "Epoch [192/300] Train Loss: 0.1016 Val Loss: 0.0605 Val Acc: 0.9849\n",
      "Epoch [193/300] Train Loss: 0.1074 Val Loss: 0.0609 Val Acc: 0.9855\n",
      "Early stopping counter: 1/15\n",
      "Epoch [194/300] Train Loss: 0.1093 Val Loss: 0.0602 Val Acc: 0.9855\n",
      "Epoch [195/300] Train Loss: 0.1052 Val Loss: 0.0599 Val Acc: 0.9849\n",
      "Epoch [196/300] Train Loss: 0.1033 Val Loss: 0.0596 Val Acc: 0.9849\n",
      "Epoch [197/300] Train Loss: 0.1059 Val Loss: 0.0606 Val Acc: 0.9844\n",
      "Early stopping counter: 1/15\n",
      "Epoch [198/300] Train Loss: 0.1048 Val Loss: 0.0596 Val Acc: 0.9857\n",
      "Early stopping counter: 2/15\n",
      "Epoch [199/300] Train Loss: 0.1036 Val Loss: 0.0594 Val Acc: 0.9857\n",
      "Epoch [200/300] Train Loss: 0.1015 Val Loss: 0.0590 Val Acc: 0.9855\n",
      "Epoch [201/300] Train Loss: 0.1029 Val Loss: 0.0594 Val Acc: 0.9852\n",
      "Early stopping counter: 1/15\n",
      "Epoch [202/300] Train Loss: 0.0995 Val Loss: 0.0594 Val Acc: 0.9855\n",
      "Early stopping counter: 2/15\n",
      "Epoch [203/300] Train Loss: 0.0984 Val Loss: 0.0589 Val Acc: 0.9849\n",
      "Epoch [204/300] Train Loss: 0.1032 Val Loss: 0.0588 Val Acc: 0.9852\n",
      "Epoch [205/300] Train Loss: 0.0993 Val Loss: 0.0587 Val Acc: 0.9860\n",
      "Epoch [206/300] Train Loss: 0.0983 Val Loss: 0.0589 Val Acc: 0.9855\n",
      "Early stopping counter: 1/15\n",
      "Epoch [207/300] Train Loss: 0.1028 Val Loss: 0.0585 Val Acc: 0.9849\n",
      "Epoch [208/300] Train Loss: 0.0992 Val Loss: 0.0584 Val Acc: 0.9862\n",
      "Epoch [209/300] Train Loss: 0.0955 Val Loss: 0.0580 Val Acc: 0.9849\n",
      "Epoch [210/300] Train Loss: 0.0979 Val Loss: 0.0587 Val Acc: 0.9860\n",
      "Early stopping counter: 1/15\n",
      "Epoch [211/300] Train Loss: 0.0971 Val Loss: 0.0580 Val Acc: 0.9865\n",
      "Early stopping counter: 2/15\n",
      "Epoch [212/300] Train Loss: 0.0969 Val Loss: 0.0581 Val Acc: 0.9857\n",
      "Early stopping counter: 3/15\n",
      "Epoch [213/300] Train Loss: 0.0977 Val Loss: 0.0579 Val Acc: 0.9855\n",
      "Epoch [214/300] Train Loss: 0.1010 Val Loss: 0.0582 Val Acc: 0.9857\n",
      "Early stopping counter: 1/15\n",
      "Epoch [215/300] Train Loss: 0.0962 Val Loss: 0.0572 Val Acc: 0.9868\n",
      "Epoch [216/300] Train Loss: 0.0987 Val Loss: 0.0578 Val Acc: 0.9862\n",
      "Early stopping counter: 1/15\n",
      "Epoch [217/300] Train Loss: 0.0995 Val Loss: 0.0574 Val Acc: 0.9865\n",
      "Early stopping counter: 2/15\n",
      "Epoch [218/300] Train Loss: 0.0962 Val Loss: 0.0575 Val Acc: 0.9865\n",
      "Early stopping counter: 3/15\n",
      "Epoch [219/300] Train Loss: 0.0988 Val Loss: 0.0577 Val Acc: 0.9865\n",
      "Early stopping counter: 4/15\n",
      "Epoch [220/300] Train Loss: 0.1005 Val Loss: 0.0570 Val Acc: 0.9865\n",
      "Epoch [221/300] Train Loss: 0.0961 Val Loss: 0.0574 Val Acc: 0.9865\n",
      "Early stopping counter: 1/15\n",
      "Epoch [222/300] Train Loss: 0.0951 Val Loss: 0.0572 Val Acc: 0.9852\n",
      "Early stopping counter: 2/15\n",
      "Epoch [223/300] Train Loss: 0.0976 Val Loss: 0.0570 Val Acc: 0.9862\n",
      "Epoch [224/300] Train Loss: 0.0933 Val Loss: 0.0569 Val Acc: 0.9860\n",
      "Epoch [225/300] Train Loss: 0.0935 Val Loss: 0.0566 Val Acc: 0.9862\n",
      "Epoch [226/300] Train Loss: 0.0954 Val Loss: 0.0565 Val Acc: 0.9862\n",
      "Epoch [227/300] Train Loss: 0.0961 Val Loss: 0.0566 Val Acc: 0.9857\n",
      "Early stopping counter: 1/15\n",
      "Epoch [228/300] Train Loss: 0.0909 Val Loss: 0.0563 Val Acc: 0.9868\n",
      "Epoch [229/300] Train Loss: 0.0903 Val Loss: 0.0563 Val Acc: 0.9862\n",
      "Epoch [230/300] Train Loss: 0.0914 Val Loss: 0.0561 Val Acc: 0.9868\n",
      "Epoch [231/300] Train Loss: 0.0916 Val Loss: 0.0562 Val Acc: 0.9865\n",
      "Early stopping counter: 1/15\n",
      "Epoch [232/300] Train Loss: 0.0895 Val Loss: 0.0563 Val Acc: 0.9857\n",
      "Early stopping counter: 2/15\n",
      "Epoch [233/300] Train Loss: 0.0903 Val Loss: 0.0559 Val Acc: 0.9860\n",
      "Epoch [234/300] Train Loss: 0.0924 Val Loss: 0.0554 Val Acc: 0.9868\n",
      "Epoch [235/300] Train Loss: 0.0928 Val Loss: 0.0559 Val Acc: 0.9868\n",
      "Early stopping counter: 1/15\n",
      "Epoch [236/300] Train Loss: 0.0920 Val Loss: 0.0556 Val Acc: 0.9865\n",
      "Early stopping counter: 2/15\n",
      "Epoch [237/300] Train Loss: 0.0919 Val Loss: 0.0554 Val Acc: 0.9862\n",
      "Early stopping counter: 3/15\n",
      "Epoch [238/300] Train Loss: 0.0907 Val Loss: 0.0558 Val Acc: 0.9862\n",
      "Early stopping counter: 4/15\n",
      "Epoch [239/300] Train Loss: 0.0932 Val Loss: 0.0557 Val Acc: 0.9868\n",
      "Early stopping counter: 5/15\n",
      "Epoch [240/300] Train Loss: 0.0915 Val Loss: 0.0559 Val Acc: 0.9868\n",
      "Early stopping counter: 6/15\n",
      "Epoch [241/300] Train Loss: 0.0920 Val Loss: 0.0559 Val Acc: 0.9857\n",
      "Early stopping counter: 7/15\n",
      "Epoch [242/300] Train Loss: 0.0901 Val Loss: 0.0547 Val Acc: 0.9865\n",
      "Epoch [243/300] Train Loss: 0.0923 Val Loss: 0.0549 Val Acc: 0.9857\n",
      "Early stopping counter: 1/15\n",
      "Epoch [244/300] Train Loss: 0.0892 Val Loss: 0.0542 Val Acc: 0.9870\n",
      "Epoch [245/300] Train Loss: 0.0880 Val Loss: 0.0542 Val Acc: 0.9868\n",
      "Epoch [246/300] Train Loss: 0.0947 Val Loss: 0.0546 Val Acc: 0.9862\n",
      "Early stopping counter: 1/15\n",
      "Epoch [247/300] Train Loss: 0.0883 Val Loss: 0.0545 Val Acc: 0.9865\n",
      "Early stopping counter: 2/15\n",
      "Epoch [248/300] Train Loss: 0.0878 Val Loss: 0.0538 Val Acc: 0.9870\n",
      "Epoch [249/300] Train Loss: 0.0906 Val Loss: 0.0541 Val Acc: 0.9862\n",
      "Early stopping counter: 1/15\n",
      "Epoch [250/300] Train Loss: 0.0909 Val Loss: 0.0546 Val Acc: 0.9862\n",
      "Early stopping counter: 2/15\n",
      "Epoch [251/300] Train Loss: 0.0913 Val Loss: 0.0542 Val Acc: 0.9860\n",
      "Early stopping counter: 3/15\n",
      "Epoch [252/300] Train Loss: 0.0893 Val Loss: 0.0550 Val Acc: 0.9860\n",
      "Early stopping counter: 4/15\n",
      "Epoch [253/300] Train Loss: 0.0883 Val Loss: 0.0538 Val Acc: 0.9865\n",
      "Epoch [254/300] Train Loss: 0.0875 Val Loss: 0.0534 Val Acc: 0.9865\n",
      "Epoch [255/300] Train Loss: 0.0852 Val Loss: 0.0533 Val Acc: 0.9870\n",
      "Epoch [256/300] Train Loss: 0.0883 Val Loss: 0.0542 Val Acc: 0.9865\n",
      "Early stopping counter: 1/15\n",
      "Epoch [257/300] Train Loss: 0.0904 Val Loss: 0.0536 Val Acc: 0.9868\n",
      "Early stopping counter: 2/15\n",
      "Epoch [258/300] Train Loss: 0.0866 Val Loss: 0.0536 Val Acc: 0.9862\n",
      "Early stopping counter: 3/15\n",
      "Epoch [259/300] Train Loss: 0.0868 Val Loss: 0.0537 Val Acc: 0.9868\n",
      "Early stopping counter: 4/15\n",
      "Epoch [260/300] Train Loss: 0.0870 Val Loss: 0.0538 Val Acc: 0.9860\n",
      "Early stopping counter: 5/15\n",
      "Epoch [261/300] Train Loss: 0.0847 Val Loss: 0.0534 Val Acc: 0.9865\n",
      "Early stopping counter: 6/15\n",
      "Epoch [262/300] Train Loss: 0.0870 Val Loss: 0.0537 Val Acc: 0.9865\n",
      "Early stopping counter: 7/15\n",
      "Epoch [263/300] Train Loss: 0.0845 Val Loss: 0.0533 Val Acc: 0.9868\n",
      "Epoch [264/300] Train Loss: 0.0823 Val Loss: 0.0537 Val Acc: 0.9865\n",
      "Early stopping counter: 1/15\n",
      "Epoch [265/300] Train Loss: 0.0895 Val Loss: 0.0531 Val Acc: 0.9870\n",
      "Epoch [266/300] Train Loss: 0.0851 Val Loss: 0.0531 Val Acc: 0.9870\n",
      "Epoch [267/300] Train Loss: 0.0834 Val Loss: 0.0531 Val Acc: 0.9868\n",
      "Early stopping counter: 1/15\n",
      "Epoch [268/300] Train Loss: 0.0844 Val Loss: 0.0525 Val Acc: 0.9868\n",
      "Epoch [269/300] Train Loss: 0.0853 Val Loss: 0.0526 Val Acc: 0.9862\n",
      "Early stopping counter: 1/15\n",
      "Epoch [270/300] Train Loss: 0.0813 Val Loss: 0.0523 Val Acc: 0.9875\n",
      "Epoch [271/300] Train Loss: 0.0822 Val Loss: 0.0527 Val Acc: 0.9870\n",
      "Early stopping counter: 1/15\n",
      "Epoch [272/300] Train Loss: 0.0840 Val Loss: 0.0529 Val Acc: 0.9870\n",
      "Early stopping counter: 2/15\n",
      "Epoch [273/300] Train Loss: 0.0873 Val Loss: 0.0534 Val Acc: 0.9868\n",
      "Early stopping counter: 3/15\n",
      "Epoch [274/300] Train Loss: 0.0863 Val Loss: 0.0521 Val Acc: 0.9868\n",
      "Epoch [275/300] Train Loss: 0.0822 Val Loss: 0.0525 Val Acc: 0.9868\n",
      "Early stopping counter: 1/15\n",
      "Epoch [276/300] Train Loss: 0.0823 Val Loss: 0.0526 Val Acc: 0.9875\n",
      "Early stopping counter: 2/15\n",
      "Epoch [277/300] Train Loss: 0.0851 Val Loss: 0.0522 Val Acc: 0.9865\n",
      "Early stopping counter: 3/15\n",
      "Epoch [278/300] Train Loss: 0.0797 Val Loss: 0.0517 Val Acc: 0.9870\n",
      "Epoch [279/300] Train Loss: 0.0837 Val Loss: 0.0523 Val Acc: 0.9870\n",
      "Early stopping counter: 1/15\n",
      "Epoch [280/300] Train Loss: 0.0833 Val Loss: 0.0520 Val Acc: 0.9873\n",
      "Early stopping counter: 2/15\n",
      "Epoch [281/300] Train Loss: 0.0806 Val Loss: 0.0520 Val Acc: 0.9873\n",
      "Early stopping counter: 3/15\n",
      "Epoch [282/300] Train Loss: 0.0829 Val Loss: 0.0517 Val Acc: 0.9870\n",
      "Epoch [283/300] Train Loss: 0.0825 Val Loss: 0.0521 Val Acc: 0.9865\n",
      "Early stopping counter: 1/15\n",
      "Epoch [284/300] Train Loss: 0.0811 Val Loss: 0.0521 Val Acc: 0.9870\n",
      "Early stopping counter: 2/15\n",
      "Epoch [285/300] Train Loss: 0.0818 Val Loss: 0.0523 Val Acc: 0.9873\n",
      "Early stopping counter: 3/15\n",
      "Epoch [286/300] Train Loss: 0.0810 Val Loss: 0.0514 Val Acc: 0.9873\n",
      "Epoch [287/300] Train Loss: 0.0787 Val Loss: 0.0520 Val Acc: 0.9875\n",
      "Early stopping counter: 1/15\n",
      "Epoch [288/300] Train Loss: 0.0790 Val Loss: 0.0515 Val Acc: 0.9873\n",
      "Early stopping counter: 2/15\n",
      "Epoch [289/300] Train Loss: 0.0800 Val Loss: 0.0515 Val Acc: 0.9865\n",
      "Early stopping counter: 3/15\n",
      "Epoch [290/300] Train Loss: 0.0846 Val Loss: 0.0515 Val Acc: 0.9870\n",
      "Early stopping counter: 4/15\n",
      "Epoch [291/300] Train Loss: 0.0826 Val Loss: 0.0516 Val Acc: 0.9870\n",
      "Early stopping counter: 5/15\n",
      "Epoch [292/300] Train Loss: 0.0822 Val Loss: 0.0513 Val Acc: 0.9875\n",
      "Epoch [293/300] Train Loss: 0.0811 Val Loss: 0.0510 Val Acc: 0.9878\n",
      "Epoch [294/300] Train Loss: 0.0788 Val Loss: 0.0520 Val Acc: 0.9862\n",
      "Early stopping counter: 1/15\n",
      "Epoch [295/300] Train Loss: 0.0799 Val Loss: 0.0522 Val Acc: 0.9860\n",
      "Early stopping counter: 2/15\n",
      "Epoch [296/300] Train Loss: 0.0829 Val Loss: 0.0519 Val Acc: 0.9865\n",
      "Early stopping counter: 3/15\n",
      "Epoch [297/300] Train Loss: 0.0815 Val Loss: 0.0512 Val Acc: 0.9868\n",
      "Early stopping counter: 4/15\n",
      "Epoch [298/300] Train Loss: 0.0793 Val Loss: 0.0509 Val Acc: 0.9870\n",
      "Epoch [299/300] Train Loss: 0.0796 Val Loss: 0.0511 Val Acc: 0.9868\n",
      "Early stopping counter: 1/15\n",
      "Epoch [300/300] Train Loss: 0.0789 Val Loss: 0.0507 Val Acc: 0.9870\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=128)\n",
    "\n",
    "num_epochs = 300\n",
    "patience = 15  # number of epochs to wait\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # ----- Validation -----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += yb.size(0)\n",
    "            correct += (predicted == yb).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f} \"\n",
    "          f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # ----- Early Stopping Logic -----\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"Early stopping counter: {counter}/{patience}\")\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045176c3",
   "metadata": {},
   "source": [
    "## Final Test Evaluation\n",
    "\n",
    "After training finishes, I load the best saved model and evaluate it on the test set.\n",
    "\n",
    "The test set was never used during training or validation.  \n",
    "Therefore, it provides an unbiased estimate of the model’s generalization performance.\n",
    "\n",
    "Evaluation metrics include:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score\n",
    "\n",
    "Accuracy is calculated as:\n",
    "\n",
    "Accuracy = Correct Predictions / Total Samples\n",
    "\n",
    "These metrics give a complete view of classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2741f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9846832814122534\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=128)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        outputs = model(xb)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Accuracy\n",
    "test_accuracy = (all_preds == all_labels).mean()\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "43faad0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "           call       0.99      0.98      0.98       226\n",
      "        dislike       0.99      1.00      1.00       194\n",
      "           fist       0.99      0.98      0.99       141\n",
      "           four       0.98      0.99      0.98       245\n",
      "           like       0.98      0.99      0.99       216\n",
      "           mute       0.95      0.97      0.96       163\n",
      "             ok       0.99      0.99      0.99       239\n",
      "            one       0.98      0.96      0.97       189\n",
      "           palm       0.96      1.00      0.98       248\n",
      "          peace       1.00      0.99      0.99       216\n",
      " peace_inverted       1.00      0.99      0.99       225\n",
      "           rock       1.00      1.00      1.00       219\n",
      "           stop       0.97      0.96      0.96       223\n",
      "  stop_inverted       0.97      0.98      0.97       235\n",
      "          three       1.00      0.96      0.98       219\n",
      "         three2       1.00      1.00      1.00       248\n",
      "         two_up       0.99      0.99      0.99       201\n",
      "two_up_inverted       0.99      0.98      0.99       205\n",
      "\n",
      "       accuracy                           0.98      3852\n",
      "      macro avg       0.98      0.98      0.98      3852\n",
      "   weighted avg       0.98      0.98      0.98      3852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=le.classes_\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aacd05",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, I reimplemented a classical supervised classification problem using a Deep Neural Network.\n",
    "\n",
    "### Comparison with ML1 Baseline (SVM)\n",
    "\n",
    "| Model | Test Accuracy |\n",
    "|-------|--------------|\n",
    "| SVM (ML1) | 99.08% |\n",
    "| Deep Neural Network | 98.47% |\n",
    "\n",
    "The SVM marginally outperforms the MLP. This is actually expected and theoretically meaningful.\n",
    "\n",
    "The input consists of **hand-crafted, low-dimensional, structured features** (63 landmark coordinates) — not raw pixels or sequences. SVMs with RBF kernels are very well-suited for this regime, as they can find optimal decision boundaries in low-dimensional spaces using the kernel trick.\n",
    "\n",
    "Deep networks tend to outperform classical models when features are **raw and high-dimensional** (images, text, audio), where they learn hierarchical representations automatically through backpropagation. With only 63 features, the MLP has less room to leverage its capacity advantage.\n",
    "\n",
    "This does **not** mean Deep Learning failed — 98% accuracy across 18 gesture classes is excellent. It illustrates an important principle:\n",
    "\n",
    "> **Model selection should match the data regime. Deep Learning is not universally superior — its advantage depends on the dimensionality, volume, and structure of the data.**\n",
    "\n",
    "### Full Pipeline Summary\n",
    "\n",
    "The complete pipeline included:\n",
    "- **Preprocessing** — Label encoding and StandardScaler normalization\n",
    "- **Data Splitting** — Stratified Train / Validation / Test split (70/15/15)\n",
    "- **Model Training** — Mini-batch gradient descent with Adam optimizer\n",
    "- **Regularization** — Dropout (0.3) and Early Stopping based on validation loss\n",
    "- **Final Evaluation** — Unbiased test set evaluation with accuracy, precision, recall, and F1-score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
