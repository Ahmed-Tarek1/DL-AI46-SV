{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206a4e4a",
   "metadata": {},
   "source": [
    "# Hand Gesture Classification Using Deep Learning\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "In ML1, I worked on a multi-class hand gesture classification problem using classical machine learning models such as SVM and Random Forest.\n",
    "\n",
    "Each sample consists of 21 hand landmarks extracted from MediaPipe.  \n",
    "Each landmark has (x, y, z) coordinates, so the total number of input features is:\n",
    "\n",
    "21 × 3 = 63 features.\n",
    "\n",
    "The goal is to predict the gesture label (e.g., fist, call, dislike, etc.).\n",
    "\n",
    "This is a supervised multi-class classification problem where:\n",
    "\n",
    "X ∈ R^63  \n",
    "y ∈ {0, 1, ..., K-1}\n",
    "\n",
    "In this notebook, I reimplement the same problem using a Deep Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27996fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78b250",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before training a neural network, preprocessing is very important.\n",
    "\n",
    "1) Label Encoding  \n",
    "The neural network requires numerical class labels, so I encode the gesture names into integer values.\n",
    "\n",
    "2) Feature Standardization  \n",
    "I apply StandardScaler to normalize the features.\n",
    "\n",
    "This is important from an optimization perspective. Neural networks are trained using gradient descent. If features are on different scales, the loss surface becomes elongated, which leads to zig-zag updates and slower convergence.\n",
    "\n",
    "By standardizing the features, the gradients become more stable and training becomes faster and more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"/Users/ahmedtarek/Developer/Python/DL/hand_landmarks_data.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(\"label\", axis=1).values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Standardization (VERY important for NN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adecc141",
   "metadata": {},
   "source": [
    "## Train / Validation / Test Split\n",
    "\n",
    "The dataset is split into:\n",
    "\n",
    "- Training set → used to update model parameters.\n",
    "- Validation set → used to monitor generalization and apply early stopping.\n",
    "- Test set → used only once at the end for final evaluation.\n",
    "\n",
    "This separation ensures that the test set remains completely unseen during training.  \n",
    "It allows us to measure the true generalization performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_val = torch.LongTensor(y_val)\n",
    "\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cadf215",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "I implement a fully connected neural network (Multi-Layer Perceptron).\n",
    "\n",
    "Structure:\n",
    "- Input layer: 63 neurons (one per feature)\n",
    "- Hidden Layer 1: 128 neurons + ReLU\n",
    "- Hidden Layer 2: 64 neurons + ReLU\n",
    "- Output layer: K neurons (number of gesture classes)\n",
    "\n",
    "Why ReLU?\n",
    "\n",
    "ReLU(x) = max(0, x)\n",
    "\n",
    "ReLU helps reduce the vanishing gradient problem and allows deeper models to train more efficiently compared to sigmoid or tanh.\n",
    "\n",
    "The model learns nonlinear transformations of the input features, which allows it to capture complex decision boundaries compared to classical linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6e1ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(GestureNN, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = GestureNN(input_size=63, num_classes=len(le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8ec77",
   "metadata": {},
   "source": [
    "## Loss Function and Optimization\n",
    "\n",
    "For multi-class classification, I use CrossEntropyLoss.\n",
    "\n",
    "Cross-entropy measures the difference between predicted class probabilities and the true class label.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "L = - Σ y_i log(ŷ_i)\n",
    "\n",
    "This encourages the model to assign high probability to the correct class.\n",
    "\n",
    "For optimization, I use Adam.\n",
    "\n",
    "Adam combines:\n",
    "- Momentum (to accelerate convergence)\n",
    "- Adaptive learning rates (to handle different parameter scales)\n",
    "\n",
    "This makes training more stable and faster compared to standard gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7c89444",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986c43f0",
   "metadata": {},
   "source": [
    "## Training Process with Early Stopping\n",
    "\n",
    "During training, each batch of data goes through the following steps:\n",
    "\n",
    "1. **Forward pass** – compute predictions.  \n",
    "2. **Compute loss** – measure the difference between predictions and true labels.  \n",
    "3. **Backward pass** – compute gradients using backpropagation.  \n",
    "4. **Update weights** – adjust model parameters using the Adam optimizer according to the gradient descent rule:\n",
    "\n",
    "\\[\n",
    "\\theta = \\theta - \\alpha \\nabla L(\\theta)\n",
    "\\]\n",
    "\n",
    "Where:  \n",
    "- **θ** represents the model parameters.  \n",
    "- **α** is the learning rate.  \n",
    "- **∇L(θ)** is the gradient of the loss with respect to the parameters.  \n",
    "\n",
    "This process is repeated for multiple epochs until the model converges.\n",
    "\n",
    "### Early Stopping (Regularization)\n",
    "\n",
    "To prevent overfitting, we implement **early stopping** based on validation loss:\n",
    "\n",
    "- Training loss usually decreases continuously.  \n",
    "- Validation loss decreases initially but may start increasing once overfitting begins.  \n",
    "\n",
    "We monitor the validation loss after each epoch. If it does not improve for a fixed number of epochs (called **patience**), training stops, and the best model (with the lowest validation loss) is restored.  \n",
    "\n",
    "By selecting the parameters that minimize validation loss rather than just training loss, early stopping acts as an **implicit regularization method**, helping the model generalize better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef658189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 1.5310 Val Loss: 0.6980 Val Acc: 0.7349\n",
      "Epoch [2/100] Train Loss: 0.7206 Val Loss: 0.5262 Val Acc: 0.7967\n",
      "Epoch [3/100] Train Loss: 0.6144 Val Loss: 0.4609 Val Acc: 0.8154\n",
      "Epoch [4/100] Train Loss: 0.5490 Val Loss: 0.4082 Val Acc: 0.8364\n",
      "Epoch [5/100] Train Loss: 0.4944 Val Loss: 0.3616 Val Acc: 0.8561\n",
      "Epoch [6/100] Train Loss: 0.4492 Val Loss: 0.3260 Val Acc: 0.8699\n",
      "Epoch [7/100] Train Loss: 0.4068 Val Loss: 0.2950 Val Acc: 0.8826\n",
      "Epoch [8/100] Train Loss: 0.3804 Val Loss: 0.2816 Val Acc: 0.8876\n",
      "Epoch [9/100] Train Loss: 0.3596 Val Loss: 0.2704 Val Acc: 0.8826\n",
      "Epoch [10/100] Train Loss: 0.3430 Val Loss: 0.2643 Val Acc: 0.8907\n",
      "Epoch [11/100] Train Loss: 0.3278 Val Loss: 0.2616 Val Acc: 0.8860\n",
      "Epoch [12/100] Train Loss: 0.3263 Val Loss: 0.2372 Val Acc: 0.9031\n",
      "Epoch [13/100] Train Loss: 0.3193 Val Loss: 0.2443 Val Acc: 0.9018\n",
      "Early stopping counter: 1/10\n",
      "Epoch [14/100] Train Loss: 0.3037 Val Loss: 0.2303 Val Acc: 0.9078\n",
      "Epoch [15/100] Train Loss: 0.3003 Val Loss: 0.2226 Val Acc: 0.9091\n",
      "Epoch [16/100] Train Loss: 0.2906 Val Loss: 0.2274 Val Acc: 0.9024\n",
      "Early stopping counter: 1/10\n",
      "Epoch [17/100] Train Loss: 0.3018 Val Loss: 0.2274 Val Acc: 0.9146\n",
      "Early stopping counter: 2/10\n",
      "Epoch [18/100] Train Loss: 0.2939 Val Loss: 0.2144 Val Acc: 0.9187\n",
      "Epoch [19/100] Train Loss: 0.2874 Val Loss: 0.2179 Val Acc: 0.9016\n",
      "Early stopping counter: 1/10\n",
      "Epoch [20/100] Train Loss: 0.2739 Val Loss: 0.2124 Val Acc: 0.9130\n",
      "Epoch [21/100] Train Loss: 0.2778 Val Loss: 0.2147 Val Acc: 0.9109\n",
      "Early stopping counter: 1/10\n",
      "Epoch [22/100] Train Loss: 0.2774 Val Loss: 0.2029 Val Acc: 0.9192\n",
      "Epoch [23/100] Train Loss: 0.2616 Val Loss: 0.1997 Val Acc: 0.9213\n",
      "Epoch [24/100] Train Loss: 0.2665 Val Loss: 0.2044 Val Acc: 0.9169\n",
      "Early stopping counter: 1/10\n",
      "Epoch [25/100] Train Loss: 0.2643 Val Loss: 0.1972 Val Acc: 0.9218\n",
      "Epoch [26/100] Train Loss: 0.2558 Val Loss: 0.1799 Val Acc: 0.9364\n",
      "Epoch [27/100] Train Loss: 0.2505 Val Loss: 0.1788 Val Acc: 0.9353\n",
      "Epoch [28/100] Train Loss: 0.2512 Val Loss: 0.2027 Val Acc: 0.9260\n",
      "Early stopping counter: 1/10\n",
      "Epoch [29/100] Train Loss: 0.2443 Val Loss: 0.1782 Val Acc: 0.9348\n",
      "Epoch [30/100] Train Loss: 0.2340 Val Loss: 0.1700 Val Acc: 0.9353\n",
      "Epoch [31/100] Train Loss: 0.2424 Val Loss: 0.1846 Val Acc: 0.9317\n",
      "Early stopping counter: 1/10\n",
      "Epoch [32/100] Train Loss: 0.2313 Val Loss: 0.1623 Val Acc: 0.9426\n",
      "Epoch [33/100] Train Loss: 0.2267 Val Loss: 0.1699 Val Acc: 0.9304\n",
      "Early stopping counter: 1/10\n",
      "Epoch [34/100] Train Loss: 0.2272 Val Loss: 0.1513 Val Acc: 0.9462\n",
      "Epoch [35/100] Train Loss: 0.2280 Val Loss: 0.1528 Val Acc: 0.9483\n",
      "Early stopping counter: 1/10\n",
      "Epoch [36/100] Train Loss: 0.2182 Val Loss: 0.1470 Val Acc: 0.9512\n",
      "Epoch [37/100] Train Loss: 0.2158 Val Loss: 0.1594 Val Acc: 0.9431\n",
      "Early stopping counter: 1/10\n",
      "Epoch [38/100] Train Loss: 0.2030 Val Loss: 0.1369 Val Acc: 0.9546\n",
      "Epoch [39/100] Train Loss: 0.2006 Val Loss: 0.1436 Val Acc: 0.9527\n",
      "Early stopping counter: 1/10\n",
      "Epoch [40/100] Train Loss: 0.2056 Val Loss: 0.1298 Val Acc: 0.9546\n",
      "Epoch [41/100] Train Loss: 0.1988 Val Loss: 0.1370 Val Acc: 0.9535\n",
      "Early stopping counter: 1/10\n",
      "Epoch [42/100] Train Loss: 0.2015 Val Loss: 0.1427 Val Acc: 0.9512\n",
      "Early stopping counter: 2/10\n",
      "Epoch [43/100] Train Loss: 0.1902 Val Loss: 0.1144 Val Acc: 0.9681\n",
      "Epoch [44/100] Train Loss: 0.1872 Val Loss: 0.1223 Val Acc: 0.9585\n",
      "Early stopping counter: 1/10\n",
      "Epoch [45/100] Train Loss: 0.1845 Val Loss: 0.1327 Val Acc: 0.9569\n",
      "Early stopping counter: 2/10\n",
      "Epoch [46/100] Train Loss: 0.1829 Val Loss: 0.1141 Val Acc: 0.9634\n",
      "Epoch [47/100] Train Loss: 0.1767 Val Loss: 0.1132 Val Acc: 0.9694\n",
      "Epoch [48/100] Train Loss: 0.1751 Val Loss: 0.1192 Val Acc: 0.9626\n",
      "Early stopping counter: 1/10\n",
      "Epoch [49/100] Train Loss: 0.1707 Val Loss: 0.1081 Val Acc: 0.9681\n",
      "Epoch [50/100] Train Loss: 0.1751 Val Loss: 0.1024 Val Acc: 0.9678\n",
      "Epoch [51/100] Train Loss: 0.1593 Val Loss: 0.0900 Val Acc: 0.9771\n",
      "Epoch [52/100] Train Loss: 0.1671 Val Loss: 0.0986 Val Acc: 0.9665\n",
      "Early stopping counter: 1/10\n",
      "Epoch [53/100] Train Loss: 0.1558 Val Loss: 0.0943 Val Acc: 0.9701\n",
      "Early stopping counter: 2/10\n",
      "Epoch [54/100] Train Loss: 0.1568 Val Loss: 0.0912 Val Acc: 0.9727\n",
      "Early stopping counter: 3/10\n",
      "Epoch [55/100] Train Loss: 0.1448 Val Loss: 0.0956 Val Acc: 0.9717\n",
      "Early stopping counter: 4/10\n",
      "Epoch [56/100] Train Loss: 0.1555 Val Loss: 0.0804 Val Acc: 0.9787\n",
      "Epoch [57/100] Train Loss: 0.1491 Val Loss: 0.0859 Val Acc: 0.9764\n",
      "Early stopping counter: 1/10\n",
      "Epoch [58/100] Train Loss: 0.1468 Val Loss: 0.0778 Val Acc: 0.9797\n",
      "Epoch [59/100] Train Loss: 0.1393 Val Loss: 0.0787 Val Acc: 0.9753\n",
      "Early stopping counter: 1/10\n",
      "Epoch [60/100] Train Loss: 0.1372 Val Loss: 0.0817 Val Acc: 0.9753\n",
      "Early stopping counter: 2/10\n",
      "Epoch [61/100] Train Loss: 0.1382 Val Loss: 0.0783 Val Acc: 0.9777\n",
      "Early stopping counter: 3/10\n",
      "Epoch [62/100] Train Loss: 0.1382 Val Loss: 0.0928 Val Acc: 0.9707\n",
      "Early stopping counter: 4/10\n",
      "Epoch [63/100] Train Loss: 0.1408 Val Loss: 0.0988 Val Acc: 0.9707\n",
      "Early stopping counter: 5/10\n",
      "Epoch [64/100] Train Loss: 0.1309 Val Loss: 0.0794 Val Acc: 0.9748\n",
      "Early stopping counter: 6/10\n",
      "Epoch [65/100] Train Loss: 0.1333 Val Loss: 0.0900 Val Acc: 0.9735\n",
      "Early stopping counter: 7/10\n",
      "Epoch [66/100] Train Loss: 0.1384 Val Loss: 0.0745 Val Acc: 0.9797\n",
      "Epoch [67/100] Train Loss: 0.1344 Val Loss: 0.0737 Val Acc: 0.9771\n",
      "Epoch [68/100] Train Loss: 0.1270 Val Loss: 0.0804 Val Acc: 0.9738\n",
      "Early stopping counter: 1/10\n",
      "Epoch [69/100] Train Loss: 0.1228 Val Loss: 0.0907 Val Acc: 0.9738\n",
      "Early stopping counter: 2/10\n",
      "Epoch [70/100] Train Loss: 0.1260 Val Loss: 0.0771 Val Acc: 0.9779\n",
      "Early stopping counter: 3/10\n",
      "Epoch [71/100] Train Loss: 0.1304 Val Loss: 0.0768 Val Acc: 0.9792\n",
      "Early stopping counter: 4/10\n",
      "Epoch [72/100] Train Loss: 0.1307 Val Loss: 0.0723 Val Acc: 0.9800\n",
      "Epoch [73/100] Train Loss: 0.1258 Val Loss: 0.0740 Val Acc: 0.9805\n",
      "Early stopping counter: 1/10\n",
      "Epoch [74/100] Train Loss: 0.1255 Val Loss: 0.0708 Val Acc: 0.9797\n",
      "Epoch [75/100] Train Loss: 0.1266 Val Loss: 0.0735 Val Acc: 0.9769\n",
      "Early stopping counter: 1/10\n",
      "Epoch [76/100] Train Loss: 0.1180 Val Loss: 0.0709 Val Acc: 0.9821\n",
      "Early stopping counter: 2/10\n",
      "Epoch [77/100] Train Loss: 0.1231 Val Loss: 0.0718 Val Acc: 0.9816\n",
      "Early stopping counter: 3/10\n",
      "Epoch [78/100] Train Loss: 0.1162 Val Loss: 0.0824 Val Acc: 0.9753\n",
      "Early stopping counter: 4/10\n",
      "Epoch [79/100] Train Loss: 0.1257 Val Loss: 0.0663 Val Acc: 0.9805\n",
      "Epoch [80/100] Train Loss: 0.1216 Val Loss: 0.0893 Val Acc: 0.9753\n",
      "Early stopping counter: 1/10\n",
      "Epoch [81/100] Train Loss: 0.1223 Val Loss: 0.0728 Val Acc: 0.9792\n",
      "Early stopping counter: 2/10\n",
      "Epoch [82/100] Train Loss: 0.1121 Val Loss: 0.0798 Val Acc: 0.9769\n",
      "Early stopping counter: 3/10\n",
      "Epoch [83/100] Train Loss: 0.1180 Val Loss: 0.0744 Val Acc: 0.9795\n",
      "Early stopping counter: 4/10\n",
      "Epoch [84/100] Train Loss: 0.1133 Val Loss: 0.0658 Val Acc: 0.9797\n",
      "Epoch [85/100] Train Loss: 0.1174 Val Loss: 0.0650 Val Acc: 0.9813\n",
      "Epoch [86/100] Train Loss: 0.1147 Val Loss: 0.0663 Val Acc: 0.9787\n",
      "Early stopping counter: 1/10\n",
      "Epoch [87/100] Train Loss: 0.1160 Val Loss: 0.0718 Val Acc: 0.9810\n",
      "Early stopping counter: 2/10\n",
      "Epoch [88/100] Train Loss: 0.1144 Val Loss: 0.0697 Val Acc: 0.9813\n",
      "Early stopping counter: 3/10\n",
      "Epoch [89/100] Train Loss: 0.1135 Val Loss: 0.0832 Val Acc: 0.9756\n",
      "Early stopping counter: 4/10\n",
      "Epoch [90/100] Train Loss: 0.1159 Val Loss: 0.0712 Val Acc: 0.9790\n",
      "Early stopping counter: 5/10\n",
      "Epoch [91/100] Train Loss: 0.1122 Val Loss: 0.0777 Val Acc: 0.9779\n",
      "Early stopping counter: 6/10\n",
      "Epoch [92/100] Train Loss: 0.1162 Val Loss: 0.0613 Val Acc: 0.9826\n",
      "Epoch [93/100] Train Loss: 0.1130 Val Loss: 0.0699 Val Acc: 0.9790\n",
      "Early stopping counter: 1/10\n",
      "Epoch [94/100] Train Loss: 0.1085 Val Loss: 0.0795 Val Acc: 0.9771\n",
      "Early stopping counter: 2/10\n",
      "Epoch [95/100] Train Loss: 0.1107 Val Loss: 0.0669 Val Acc: 0.9823\n",
      "Early stopping counter: 3/10\n",
      "Epoch [96/100] Train Loss: 0.1131 Val Loss: 0.0721 Val Acc: 0.9792\n",
      "Early stopping counter: 4/10\n",
      "Epoch [97/100] Train Loss: 0.1128 Val Loss: 0.0646 Val Acc: 0.9847\n",
      "Early stopping counter: 5/10\n",
      "Epoch [98/100] Train Loss: 0.1128 Val Loss: 0.0610 Val Acc: 0.9831\n",
      "Epoch [99/100] Train Loss: 0.1100 Val Loss: 0.0621 Val Acc: 0.9821\n",
      "Early stopping counter: 1/10\n",
      "Epoch [100/100] Train Loss: 0.1087 Val Loss: 0.0633 Val Acc: 0.9839\n",
      "Early stopping counter: 2/10\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32)\n",
    "\n",
    "num_epochs = 100\n",
    "patience = 10  # number of epochs to wait\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # ----- Validation -----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += yb.size(0)\n",
    "            correct += (predicted == yb).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f} \"\n",
    "          f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # ----- Early Stopping Logic -----\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"Early stopping counter: {counter}/{patience}\")\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045176c3",
   "metadata": {},
   "source": [
    "## Final Test Evaluation\n",
    "\n",
    "After training finishes, I load the best saved model and evaluate it on the test set.\n",
    "\n",
    "The test set was never used during training or validation.  \n",
    "Therefore, it provides an unbiased estimate of the model’s generalization performance.\n",
    "\n",
    "Evaluation metrics include:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score\n",
    "\n",
    "Accuracy is calculated as:\n",
    "\n",
    "Accuracy = Correct Predictions / Total Samples\n",
    "\n",
    "These metrics give a complete view of classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2741f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9800103842159917\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        outputs = model(xb)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Accuracy\n",
    "test_accuracy = (all_preds == all_labels).mean()\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43faad0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "           call       0.99      0.98      0.98       226\n",
      "        dislike       0.99      1.00      0.99       194\n",
      "           fist       1.00      0.99      0.99       141\n",
      "           four       0.98      1.00      0.99       245\n",
      "           like       0.98      0.99      0.98       216\n",
      "           mute       0.94      0.94      0.94       163\n",
      "             ok       1.00      1.00      1.00       239\n",
      "            one       0.97      0.95      0.96       189\n",
      "           palm       0.96      0.99      0.98       248\n",
      "          peace       0.99      0.98      0.98       216\n",
      " peace_inverted       0.99      0.96      0.98       225\n",
      "           rock       1.00      0.99      0.99       219\n",
      "           stop       0.95      0.96      0.95       223\n",
      "  stop_inverted       0.99      0.98      0.99       235\n",
      "          three       0.99      0.96      0.98       219\n",
      "         three2       1.00      1.00      1.00       248\n",
      "         two_up       0.97      0.99      0.98       201\n",
      "two_up_inverted       0.96      0.98      0.97       205\n",
      "\n",
      "       accuracy                           0.98      3852\n",
      "      macro avg       0.98      0.98      0.98      3852\n",
      "   weighted avg       0.98      0.98      0.98      3852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    all_preds,\n",
    "    target_names=le.classes_\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aacd05",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, I reimplemented a classical supervised classification problem using a Deep Neural Network.\n",
    "\n",
    "Compared to classical ML models:\n",
    "- The neural network learns nonlinear feature representations.\n",
    "- It uses backpropagation for optimization.\n",
    "- It can model more complex decision boundaries.\n",
    "\n",
    "The full pipeline included:\n",
    "- Preprocessing\n",
    "- Proper data splitting\n",
    "- Model training\n",
    "- Early stopping\n",
    "- Final unbiased evaluation\n",
    "\n",
    "This demonstrates how a classical ML problem can be effectively solved using a Deep Learning approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
